{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NextStat + Gymnasium: RL Agent Optimising Analysis Cuts\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nextstat/nextstat.io/blob/main/notebooks/02_gymnasium_rl_agent.ipynb)\n",
    "\n",
    "This notebook trains a **Reinforcement Learning agent** (PPO via Stable-Baselines3) to optimise a signal histogram's yields, maximising discovery significance.\n",
    "\n",
    "NextStat's `nextstat.gym` module wraps a HistFactory workspace as a **Gymnasium environment**:\n",
    "- **Observation**: current signal histogram yields\n",
    "- **Action**: multiplicative updates to each bin\n",
    "- **Reward**: change in Z₀ (discovery significance)\n",
    "\n",
    "The agent learns to reshape the signal histogram to maximise the statistical test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q nextstat gymnasium stable-baselines3 numpy matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "N_BINS = 8\n",
    "edges = np.linspace(0.0, 1.0, N_BINS + 1)\n",
    "centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "width = edges[1] - edges[0]\n",
    "\n",
    "signal = 40.0 * np.exp(-0.5 * ((centers - 0.5) / 0.12) ** 2) * width\n",
    "background = 200.0 * np.exp(-2.0 * centers) * width\n",
    "\n",
    "workspace = {\n",
    "    \"channels\": [{\n",
    "        \"name\": \"SR\",\n",
    "        \"samples\": [\n",
    "            {\n",
    "                \"name\": \"signal\",\n",
    "                \"data\": signal.tolist(),\n",
    "                \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"background\",\n",
    "                \"data\": background.tolist(),\n",
    "                \"modifiers\": [\n",
    "                    {\"name\": \"bkg_norm\", \"type\": \"normsys\",\n",
    "                     \"data\": {\"hi\": 1.08, \"lo\": 0.92}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    "    \"observations\": [{\"name\": \"SR\", \"data\": (signal + background).tolist()}],\n",
    "    \"measurements\": [{\n",
    "        \"name\": \"meas\",\n",
    "        \"config\": {\"poi\": \"mu\", \"parameters\": []},\n",
    "    }],\n",
    "    \"version\": \"1.0.0\",\n",
    "}\n",
    "\n",
    "ws_json = json.dumps(workspace)\n",
    "print(f\"Signal:     {np.round(signal, 2)}\")\n",
    "print(f\"Background: {np.round(background, 2)}\")\n",
    "print(f\"S/sqrt(B):  {signal.sum() / background.sum()**0.5:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the Gymnasium Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nextstat.gym import make_histfactory_env\n",
    "\n",
    "env = make_histfactory_env(\n",
    "    ws_json,\n",
    "    channel=\"SR\",\n",
    "    sample=\"signal\",\n",
    "    reward_metric=\"q0\",       # maximise discovery test statistic\n",
    "    max_steps=64,              # episode length\n",
    "    action_scale=0.02,         # small perturbations per step\n",
    "    action_mode=\"logmul\",      # multiplicative updates in log-space\n",
    "    init_noise=0.0,            # start from nominal\n",
    ")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space:      {env.action_space}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Agent Baseline\n",
    "\n",
    "First, let's see how a random agent performs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "total_reward = 0.0\n",
    "rewards_random = []\n",
    "\n",
    "for step in range(64):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += float(reward)\n",
    "    rewards_random.append(total_reward)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Random agent — total reward: {total_reward:.3f}\")\n",
    "print(f\"Random agent — final yields:  {np.round(obs, 2)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train PPO Agent (Stable-Baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Wrap for SB3\n",
    "def make_env():\n",
    "    return make_histfactory_env(\n",
    "        ws_json,\n",
    "        channel=\"SR\",\n",
    "        sample=\"signal\",\n",
    "        reward_metric=\"q0\",\n",
    "        max_steps=64,\n",
    "        action_scale=0.02,\n",
    "        action_mode=\"logmul\",\n",
    "        init_noise=0.01,  # slight noise for exploration\n",
    "    )\n",
    "\n",
    "vec_env = make_vec_env(make_env, n_envs=4)\n",
    "\n",
    "agent = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Training PPO agent...\")\n",
    "agent.learn(total_timesteps=20_000)\n",
    "print(\"Done!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eval_env = make_env()\n",
    "obs, info = eval_env.reset(seed=123)\n",
    "total_reward_ppo = 0.0\n",
    "rewards_ppo = []\n",
    "yields_history = [obs.copy()]\n",
    "\n",
    "for step in range(64):\n",
    "    action, _ = agent.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    total_reward_ppo += float(reward)\n",
    "    rewards_ppo.append(total_reward_ppo)\n",
    "    yields_history.append(obs.copy())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"PPO agent — total reward: {total_reward_ppo:.3f}\")\n",
    "print(f\"Random agent — total reward: {total_reward:.3f}\")\n",
    "print(f\"Improvement: {(total_reward_ppo - total_reward) / abs(total_reward) * 100:.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Cumulative reward\n",
    "axes[0].plot(rewards_random, label=\"Random\", color=\"gray\", alpha=0.7)\n",
    "axes[0].plot(rewards_ppo, label=\"PPO\", color=\"#D4AF37\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Cumulative Reward\")\n",
    "axes[0].set_title(\"Reward: Random vs PPO\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.2)\n",
    "\n",
    "# Yield evolution\n",
    "yields_arr = np.array(yields_history)\n",
    "for i in range(N_BINS):\n",
    "    axes[1].plot(yields_arr[:, i], alpha=0.6, linewidth=0.8)\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Yield\")\n",
    "axes[1].set_title(\"Bin Yields Over Time\")\n",
    "axes[1].grid(alpha=0.2)\n",
    "\n",
    "# Final vs initial histogram\n",
    "axes[2].bar(centers - width * 0.2, yields_history[0], width=width * 0.35,\n",
    "            color=\"gray\", alpha=0.6, label=\"Initial\")\n",
    "axes[2].bar(centers + width * 0.2, yields_history[-1], width=width * 0.35,\n",
    "            color=\"#D4AF37\", alpha=0.8, label=\"Optimised (PPO)\")\n",
    "axes[2].set_xlabel(\"Bin Center\")\n",
    "axes[2].set_ylabel(\"Signal Yield\")\n",
    "axes[2].set_title(\"Initial vs Optimised Signal\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Systematic Impact Before/After\n",
    "\n",
    "Compare the ranking plot before and after the RL optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import nextstat\nfrom nextstat.interpret import rank_impact\n\nmodel = nextstat.from_pyhf(workspace)\n\nprint(\"Ranking (original model):\")\nfor r in rank_impact(model, top_n=5):\n    print(f\"  {r['rank']}. {r['name']:20s} impact={r['total_impact']:.4f}\")\n\n# Note: to see the impact with optimised yields, you'd rebuild\n# the workspace with the agent's final yields and re-run ranking.",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We trained a PPO agent to reshape signal histogram yields, maximising discovery significance q₀. The agent learned a policy that consistently outperforms random exploration.\n",
    "\n",
    "### Key APIs used:\n",
    "- `nextstat.gym.make_histfactory_env()` — Gymnasium environment wrapper\n",
    "- `reward_metric=\"q0\"` — reward = change in test statistic\n",
    "- `action_mode=\"logmul\"` — multiplicative actions in log-space\n",
    "- `rank_impact()` — systematic impact analysis\n",
    "\n",
    "### Next steps:\n",
    "- Try `reward_metric=\"z0\"` for significance-based reward\n",
    "- Try `reward_metric=\"qmu\"` with custom `mu_test` for exclusion limits\n",
    "- Combine with the [PyTorch notebook](./01_pytorch_significance_loss.ipynb) for end-to-end NN + RL\n",
    "- Scale up with [Optuna](https://nextstat.io/docs/optuna) for hyperparameter search over RL config"
   ]
  }
 ]
}