{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NextStat + PyTorch: Training a Classifier with Significance Loss\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nextstat/nextstat.io/blob/main/notebooks/01_pytorch_significance_loss.ipynb)\n\nThis notebook demonstrates **end-to-end differentiable training** of a neural network classifier where the loss function is the **discovery significance Z₀** — computed by NextStat's profiled likelihood engine.\n\nInstead of training with cross-entropy and *then* running a statistical test, we **differentiate through the statistical test itself**.\n\n## Pipeline\n```\nNN(features) → scores → SoftHistogram → SignificanceLoss(-Z₀) → backward → optimizer\n```\n\n### Requirements\n- `nextstat` (Rust-powered statistical inference, built with `--features cuda` or `--features metal`)\n- `torch` (PyTorch)\n- GPU required: CUDA (NVIDIA) or Metal (Apple Silicon)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q nextstat torch numpy matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a HistFactory Workspace\n",
    "\n",
    "We create a simple signal + background model with one systematic uncertainty.\n",
    "In a real analysis this would come from your ntuple processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "N_BINS = 10\n",
    "edges = np.linspace(0.0, 1.0, N_BINS + 1)\n",
    "centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "width = edges[1] - edges[0]\n",
    "\n",
    "# Signal: Gaussian peak at 0.5\n",
    "signal = 30.0 * np.exp(-0.5 * ((centers - 0.5) / 0.10) ** 2) * width\n",
    "# Background: falling exponential\n",
    "background = 150.0 * np.exp(-1.5 * centers) * width\n",
    "\n",
    "workspace = {\n",
    "    \"channels\": [{\n",
    "        \"name\": \"SR\",\n",
    "        \"samples\": [\n",
    "            {\n",
    "                \"name\": \"signal\",\n",
    "                \"data\": signal.tolist(),\n",
    "                \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"background\",\n",
    "                \"data\": background.tolist(),\n",
    "                \"modifiers\": [\n",
    "                    {\"name\": \"bkg_norm\", \"type\": \"normsys\",\n",
    "                     \"data\": {\"hi\": 1.10, \"lo\": 0.90}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    "    \"observations\": [{\"name\": \"SR\", \"data\": (signal + background).tolist()}],\n",
    "    \"measurements\": [{\n",
    "        \"name\": \"meas\",\n",
    "        \"config\": {\"poi\": \"mu\", \"parameters\": []},\n",
    "    }],\n",
    "    \"version\": \"1.0.0\",\n",
    "}\n",
    "\n",
    "print(f\"Signal bins:     {np.round(signal, 2)}\")\n",
    "print(f\"Background bins: {np.round(background, 2)}\")\n",
    "print(f\"S/B ratio:       {signal.sum() / background.sum():.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model & Create Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import nextstat\n",
    "from nextstat.torch import SignificanceLoss, SoftHistogram\n",
    "\n",
    "model = nextstat.from_pyhf(workspace)\n",
    "\n",
    "# SignificanceLoss returns -Z₀ by default (for SGD minimisation)\n",
    "loss_fn = SignificanceLoss(model, \"signal\", device=\"auto\")\n",
    "\n",
    "print(f\"Expected signal bins: {loss_fn.n_bins}\")\n",
    "print(f\"Nuisance parameters:  {loss_fn.n_params}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Simple Classifier + SoftHistogram"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Toy 2-layer NN that maps features → [0, 1] score.\"\"\"\n",
    "    def __init__(self, input_dim=5, hidden=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# Differentiable binning\n",
    "soft_hist = SoftHistogram(\n",
    "    bin_edges=torch.linspace(0.0, 1.0, N_BINS + 1),\n",
    "    bandwidth=0.05,\n",
    "    mode=\"kde\",\n",
    ")\n",
    "\n",
    "print(f\"SoftHistogram: {soft_hist.n_bins} bins, bandwidth={soft_hist._bw:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "N_SIGNAL = 500\n",
    "N_BACKGROUND = 2000\n",
    "INPUT_DIM = 5\n",
    "\n",
    "# Signal: clustered around feature-space center\n",
    "x_sig = torch.randn(N_SIGNAL, INPUT_DIM) * 0.5 + 0.5\n",
    "# Background: spread wider\n",
    "x_bkg = torch.randn(N_BACKGROUND, INPUT_DIM) * 1.0\n",
    "\n",
    "# Combine\n",
    "x_all = torch.cat([x_sig, x_bkg], dim=0)\n",
    "w_all = torch.ones(N_SIGNAL + N_BACKGROUND)  # uniform weights\n",
    "\n",
    "print(f\"Signal events:     {N_SIGNAL}\")\n",
    "print(f\"Background events: {N_BACKGROUND}\")\n",
    "print(f\"Feature dim:       {INPUT_DIM}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "classifier = SimpleClassifier(input_dim=INPUT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "x_train = x_all.to(device)\n",
    "w_train = w_all.to(device)\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward: NN → scores → soft histogram → -Z₀\n",
    "    scores = classifier(x_train)\n",
    "    histogram = soft_hist(scores, w_train)\n",
    "\n",
    "    # Scale histogram to match expected yields\n",
    "    hist_scaled = histogram * (signal.sum() / (histogram.sum().item() + 1e-10))\n",
    "\n",
    "    loss = loss_fn(hist_scaled.double().to(device))\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    z0 = -loss.item()\n",
    "    history.append(z0)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Z₀ = {z0:.3f}σ\")\n",
    "\n",
    "print(f\"\\nFinal Z₀: {history[-1]:.3f}σ\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualise Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training curve\n",
    "axes[0].plot(history, color=\"#D4AF37\", linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Z₀ (σ)\")\n",
    "axes[0].set_title(\"Discovery Significance vs Epoch\")\n",
    "axes[0].grid(alpha=0.2)\n",
    "\n",
    "# Final histogram\n",
    "with torch.no_grad():\n",
    "    scores_final = classifier(x_train)\n",
    "    hist_final = soft_hist(scores_final, w_train).cpu().numpy()\n",
    "\n",
    "axes[1].bar(centers, hist_final, width=width * 0.8, color=\"#D4AF37\", alpha=0.8, label=\"NN output\")\n",
    "axes[1].step(centers, signal, where=\"mid\", color=\"red\", linewidth=1.5, label=\"True signal\")\n",
    "axes[1].set_xlabel(\"Classifier Score\")\n",
    "axes[1].set_ylabel(\"Events\")\n",
    "axes[1].set_title(\"Learned Histogram vs True Signal\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Systematic Impact (Feature Importance)\n",
    "\n",
    "After training, check which systematics dominate:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nextstat.interpret import rank_impact\n",
    "\n",
    "ranking = rank_impact(model, top_n=10)\n",
    "for r in ranking:\n",
    "    print(f\"  {r['rank']:2d}. {r['name']:20s}  impact={r['total_impact']:.4f}  pull={r['pull']:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Log to W&B (Optional)\n",
    "\n",
    "Uncomment to enable Weights & Biases logging:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import wandb\n",
    "# from nextstat.mlops import significance_metrics\n",
    "#\n",
    "# wandb.init(project=\"nextstat-colab\")\n",
    "# for i, z0 in enumerate(history):\n",
    "#     wandb.log(significance_metrics(z0, prefix=\"train/\"), step=i)\n",
    "# wandb.finish()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We trained a neural network where the loss is **profiled discovery significance** — computed by NextStat's Rust/CUDA engine. The gradient flows from the statistical test, through the differentiable histogram, into the network weights.\n",
    "\n",
    "### Key APIs used:\n",
    "- `SignificanceLoss(model, \"signal\")` — differentiable Z₀ loss\n",
    "- `SoftHistogram(bin_edges)` — differentiable binning (KDE)\n",
    "- `rank_impact(model)` — systematic impact ranking\n",
    "\n",
    "### Next steps:\n",
    "- [ML Training Guide](https://nextstat.io/docs/ml-training)\n",
    "- [Optuna Tutorial](https://nextstat.io/docs/optuna) — optimise binning\n",
    "- [RL Notebook](./02_gymnasium_rl_agent.ipynb) — RL agent for cut optimisation"
   ]
  }
 ]
}